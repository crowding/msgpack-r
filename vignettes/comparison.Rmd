x#---
#title: Which Data Format is Fastest Data Format?
#output: rmarkdown::html_vignette
#vignette: >
#  %\VignetteEngine{knitr::rmarkdown}
#  %\VignetteIndexEntry{Which Data Format is Fastest Data Format?}
#  \usepackage[utf8]{inputenc}
#---

It's good to get data into and out of R! More compatibility between
different formats is better. But... which formats or packages are fastest?

First, we will need a reference dataset to work with. I'll pick the
NYC flights dataset since it's large enough to take some time to
process.

Next, we'll pick some metrics. Here are a few I am interested in.

* wall clock time from beginning to end of task;
* CPU process time used as recorded by the OS
* total time spent to encode (to memory)
* total time spent to decode (from memory)
* total time spent to encode, transmit to another process on the same
  computer, and decode
* total number of bytes used
* all the above but with a layer of gzip used on top of the encoding

Note that this is not a completely apples-to-apples comparison, since
the different formats have different capabilities and
purposes:

* `serialize()` is your best best for getting R objects out of
  and back into R the way you had them.
* `dump()` is your next best bet, while being a somewhat
  human-readable text format.
* JSON is widely used on the web, but because it is based on
  Javascript data structures, it doesn't easily represent all of R
  losslessly; attributes like dims and classes don't have equivalents
  in Javascript.
* CSV is ubiquitous and can be read by most anything but only represents
  tabular data. Its data types are ambiguous.
* Messagepack is similar in capabilities to JSON but is a binary format
  with more compact and faster encoding.
* Some of these packages encode data frames by column (more compact)
  and others encode data frames by row (more common with JSON APIs.)

Here's my strategy for benchmarking writing over connections. I will
write the dataset, from one forked R process to another on the same
machine. The dataset will be loaded into memory before the child
process is forked, then the child process encodes and writes to the
master process, which decodes. Timings are taken on both sides of the
connection, and we can examine the roundtrip-formatted data in the
master process afterwards.

The implementation of the benchmarking code is in file `benchmarks.R`.

```{R setup, cache=FALSE, echo=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE, autodep=TRUE, message = FALSE, warning = FALSE)
read_chunk('benchmarking.R')
```

As a test dataset we'll use `nycflights13`. Since the packages vary so
much in performance, we'll have to dynamically vary the input size. So
the function `subsample` produces an artificially shrunk or enlarged
version of the dataset according to the argument `rate`.

```{R dataset, cache=FALSE, echo=FALSE}
read_chunk('benchmarking.R')
```

We'll try several different benchmarking methods:

* `convert` is converting object to an R `raw()` and back, in memory
* `conn` also uses an in-memory conversion but with R
      `textConnection` or `rawConnection` objects.
* `file` writes to a temp file and then reads it back.
* `fifo` forks the R process and writes from one while reading
      from the other over a UNIX socket.
* `tcp` forks the R process and writes from one to the other using a
      TCP socket.
* `net` (TODO) launches an R process on a remote machine on the local
  network, sends the data there, then reads a reply.

```{R definitions, cache = FALSE, echo = FALSE, results = "hide"}
read_chunk('benchmarking.R')
``` 

Each test returns the OS-reported CPU time for the process, as well as
elapsed time for reading, writing and total. For `fifo` and `tcp`
tests, the reading and writing may happen concurrently, so that
`total.elapsed < read.elapsed + write.elapsed`.

## R serialization

`serialize` and `unserialize` produce faithful replications of R
objects including R-specific structures like closures, environments,
and attributes. But generally only R code can read it. It may be a
good candidate for communicating between R processes.

For instance:

```{R}
unserialize.inmem <- timeConvert(dataset,
                                 unserialize,
                                 function(data) serialize(data, NULL))
showTimings(unserialize.inmem, "R serialization (in memory)")
```

This is reasonably speedy, too.

However, I run into a problem if I try to transfer too large an object over
a fifo or socket and read it with `unserialize().`

```{R, error=TRUE}
unserialize.bad <- timeFifoTransfer(dataset, unserialize, serialize)
dput(unserialize.bad$extra, control=c())
```

This appears to be because `unserialize` doesn't operate concurrently,
in the sense that it doesn't recover from finding the end of the line
having only read part of a message. Meanwhile, on my machine `fifo()`
and `socketConnection()` do not seem to block even if `blocking =
TRUE` is set. They always return some number of bytes, but it may be
fewer than requested. So `unserialize` does not work with blocking
socket connections.

One workaround is to exhaustively read the connection before handing
off the data to `unserialize`,

```{R}
readAll <- function(con, chunk = 8192L) {
  buf <- rawConnection(raw(0), open="w")
  repeat {
    result <- readBin(con = con, what = "raw", n = chunk)
    if (length(result) == 0) break
    writeBin(result, buf)
  }
  rawConnectionValue(buf)
}

unserialize.socket <- timeSocketTransfer(dataset,
                                         . %>% readAll %>% unserialize,
                                         serialize)
showTimings(unserialize.socket, c("R serialization (over socket, blocking read)"))
```

But this strategy only works for transmitting one object per
connection, and for one connection at a time.

Another way you can do it is to wrap R serialization with a
`messageConnection`. This only adds a few bytes to the connection, and
msgpack will handle assembling complete messages. This also allows you
to send several values per connection and poll several msgpack
connections until one of them returns a decoded message.

```{R}
unserialize.wrapped_socket <- timeSocketTransfer(
  dataset,
  reader = function(con) unserialize(msgpack::readMsg(con)),
  writer = function(x, con) msgpack::writeMsg(serialize(x, NULL), con),
  wrap = msgpack::msgConnection
)
```{R}

```{R}
showTimings(unserialize.wrapped_socket, "R serialization over msgpack over TCP")
```

(TODO: This ought to be faster.)

Now let's start collecting benchmarks systematically. Since the
methods we will explore have such a wide variance or performance, we
will test them with successively larger datasets, until they exceed a
timeout. The notation below specifies how to test R serialization and
how to label the results. See the code file [benchmarking.R] for
implementation details.

```{R serialization}
serialize.blocking <- run_tests(arg_df(c(
  common.options,
  list(
    method = list(
      convert = list(method = timeConvert),
      file = list(method = timeFileIO),
      conn = list(method = timeConnection)),
    encoder = list(
      serialize = list(
        reader = function(c) unserialize(c),
        writer = serialize,
        from = unserialize,
        to = function(data) serialize(data, NULL)))))))

serialize.readall <- run_tests(arg_df(c(
  common.options,
  list(
    method = list(
      fifo = list(method = timeFifoTransfer),
      socket = list(method = timeSocketTransfer)),
    note = list(
      "using readAll" = list()),
    encoder = list(
      serialize = list(
        reader = function(c) unserialize(readAll(c)),
        writer = serialize))))))
```

<!-- Why's it take to long to recover from an error in serialize, I
     wonder? ohhh. it's do.call with the big dataset and the traceback... -->

```{R}
benchmarks <- data.frame()
store(serialize.blocking)
store(serialize.readall)
 ```

```{R}
test.plot <- (ggplot(
  filter(benchmarks, encoder=="serialize"))
  + aes(x = size, y = total.elapsed, color = method)
  + geom_line()
  + xlab("Data size (nycflights13 = 1)")
  + ylab("Elapsed time (s)")
)
test.plot + labs(main="R serialization performance")
```

## Dput/source

The `dput` and `deparse` render R data objects to an ASCII connection
in R-like syntax.  THe idea is that the text output "looks like" the
code it takes to construct the object, to the extent that the
mechanism for reading objects back in is to `eval` or `source` the
text. (Hopefully one does not do this with untrusted data. A better
technique may be to evaluate the data in a limited environment that
just contains the needed constructors like `structure`, `list` and `c`
etc.)

```{R}
## Annoyingly, the behavior of "dump" and "dput" depend on this global option.
options(deparse.max.lines = NULL)

dput(control=c(),
     list(1, "2", verb=quote(buckle), my=c("s", "h", "o", "e")))
```

(Note how `dput` fails on transmitting language objects; if we try to
eval the above we will try to evaluate "buckle" instead of getting
just the name object.)

Performance-wise, `dput` and `source` should not be used for large
datasets, because they display an O(n^2) characteristic in terms of
the data size.

```{R dput}
options(deparse.max.lines = NULL)
dput.timings <- run_tests(arg_df(c(
  all.common.options,
  list(
    encoder = list(
      dput = list(
        from = function(t) eval(parse(text=t)),
        to = deparse,
        reader = function(c) eval(parse(c)),
        writer = function(x, c) dput(x, file=c)))))))
```

```{R, results = "hide"}
benchmarks <- store(dput.timings, benchmarks)
```

```{R}
test.plot + labs(main="dput performance") %+% filter(benchmarks, encoder == "dput")
 ```

## jsonlite

```{R jsonlite}
# `jsonlite` supports streaming reads but only of one data frame at a time.
jsonlite_reader <- function(conn) {
  append <- msgpack:::catenator()
  stream_in(conn, handler = function(x) append(list(x)))
  append(action="read")
}

jsonlite_writer <- function(l, conn) {
  lapply(l, stream_out, conn)
}

jsonlite.timings <- run_tests(arg_df(c(
  convert.common.options,
  list(
    encoder = list(
      jsonlite = list(
        to = jsonlite::toJSON,
        from = jsonlite::fromJSON,
        reader = jsonlite_reader,
        writer = jsonlite_writer))))))
```

jsonlite performs reasonably well, but is about `{R} "several"` times slower than
serializing.

```{R, results = "hide"}
benchmarks <- store(jsonlite.timings, benchmarks)
```

```{R}
test.plot + labs(main="jsonlite performance") %+% filter(benchmarks, encoder == "jsonlite")
```

## Msgpack

Msgpack is quite zippy but has some troubling quadratic behavior...

```{R msgpack}
msgpack.timings <- run_tests(arg_df(c(
  all.common.options,
  list(
    encoder = list(
      msgpack = list(
        wrap = msgpack::msgConnection,
        reader = msgpack::readMsg,
        writer = msgpack::writeMsg,
        to = msgpack::packMsg,
        from = msgpack::unpackMsg))))))
```

```{R, results = "hide"}
benchmarks <- store(msgpack.timings, benchmarks)
```

```{R}
test.plot + labs(main="msgpack performance") %+% filter(benchmarks, encoder == "msgpack")
```

Implementing the streaming-mode callback has helped a lot, but there
is still an O(N^2) characteristic going on here. Need to profile
memory allocation.

## msgpackR

There is an older pure-R implementation of msgpack on CRAN. One quirk is that
it doesn't accept `NA` in R vectors.

```{R, error = TRUE}
msgpackR::pack(c(1, 2, 3))
msgpackR::pack(c(1, 2, 3, NA))
```

As a workaround I'll substitute out all the NA values in the dataset.

```{R}
dataset_mungenull <- map(dataset, map_dfc,
                         function(col) ifelse(is.na(col), 9999, col))
```

Performance-wise, it is mostly linear but quite slow.

```{R msgpackR}
msgpackR.timings <- run_tests(arg_df(c(
  all.common.options %but% list( 
    note = list("no NAs, readAll" = list()),
    dataset = list(nycflights13 = list(data = dataset_mungenull))),
  list(
    encoder = list(
      msgpackR = list(
        from = msgpackR::unpack,
        to = msgpackR::pack,
        reader = function(conn) msgpackR::unpack(readAll(conn)),
        writer = function(data, conn) writeBin(msgpackR::pack(data), conn)))))))
```

```{R, results = "hide"}
benchmarks <- store(msgpackR.timings, benchmarks)
```

```{R}
test.plot + labs(main="msgpackR performance") %+% filter(benchmarks, encoder == "msgpackR")
```

## rjson

```{R rjson}
rjson.timings <- run_tests(arg_df(c(
  convert.common.options,
  list(
    encoder = list( 
      rjson = list(
        from = rjson::fromJSON,
        to = rjson::toJSON))))))
```

rjson is rather zippy -- only `{R "several"}` times slower than msgpack :)

```{R results="hide"}
benchmarks <- store(rjson.timings, benchmarks)
```

```{R}
test.plot + labs(main="rjson performance") %+% filter(benchmarks, encoder == "rjson")
```

## RJSONIO

```{R RJSONIO}
RJSONIO.timings <- run_tests(arg_df(c(
  all.common.options,
  list(
    encoder = list(
      RJSONIO = list(
        from = RJSONIO::fromJSON,
        to = RJSONIO::toJSON,
        reader = function(con) RJSONIO::readJSONStream(con),
        writer = function(x, con) writeBin(RJSONIO::toJSON(x), con),
        raw = TRUE))))))
```

Oddly, RJSONIO's streaming read function is quite slow.

```{R, results = "hide"}
benchmarks <- store(RJSONIO.timings, benchmarks)
```

```{R}
test.plot + labs(main="RJSONIO performance") %+% filter(benchmarks, encoder == "RJSONIO")
```

## ndjson

ndjson is a "wicked-fast" streaming JSON reader, but not a
writer. So I will pair it with the fastest streaming JSON writer.

## write.csv

Do we really have to go here? I suppose...

## XML

## YAML

Oh dear.

```{R}
save(benchmarks, file="benchmarks.RData")
```
 
