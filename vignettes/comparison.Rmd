#---
#title: The Fastest Way to get data in to or out of R?
#output: rmarkdown::html_vignette
#vignette: >
#  %\VignetteEngine{knitr::rmarkdown}
#  %\VignetteIndexEntry{Speed of R Import and Export data in Various Packages}
#  \usepackage[utf8]{inputenc}
#---

It's good to get data into and out of R! More compatibility between
different formats is better. But... which formats or packages are fastest?

First, we will need a reference dataset to work with. I'll pick the
NYC flights dataset since it's large enough to take some time to
process.

Next, we'll pick some metrics. Here are a few I am interested in.

* wall clock time from beginning to end of task;
* CPU process time used as recorded by the OS
* total time spent to encode (to memory)
* total time spent to decode (from memory)
* total time spent to encode, transmit to another process on the same
  computer, and decode
* total number of bytes used
* all the above but with a layer of gzip used on top of the encoding

Here are all the libraries we will deal with. But since some of the
names clobber others I will have to load and install them in segments.

NOTE: also investigate YAML read/write, and HTML read/write?

Note that this is not a completely apples-to-apples comparison, since
the different formats have different capabilities and
purposes:

* `serialize()` is your best best for getting R objects out of
  and back into R the way you had them.
* `dump()` is your next best bet, while being a somewhat
  human-readable text format.
* JSON is widely used on the web, but because it is based on
  Javascript data structures, it doesn't easily represent all of R
  losslessly; attributes like dims and classes don't have equivalents
  in Javascript.
* CSV is ubiquitous and can be read by most anything but only represents
  tabular data. Its data types are ambiguous.
* Msgpack is similar in capabilities to JSON but is a binary format
  with more compact and faster encoding.
* Some of these packages encode data frames by column (more compact)
  and others encode data frames by row (more common with JSON APIs.)

Here's my strategy for benchmarking writing over connections. I will
write the dataset, from one forked R process to another on the same
machine. The dataset will be loaded into memory before the child
process is forked, then the child process encodes and writes to the
master process, which decodes. Timings are taken on both sides of the
connection, and we can examine the roundtrip-formatted data in the
master process afterwards.

The implementation of the benchmarking code is in file `benchmarks.R`.

```{R external-code, cache=FALSE}
read_chunk('benchmarking.R')
```

Here's the test dataset we will use, one of the larger data packages
available on CRAN.

```{R}
library(nycflights13)

dataset <- as.list(as.environment("package:nycflights13"))
timings <- list()

subsample <- function(dataset, rate) {
    lapply(dataset, function(df) {
        df[1:(1 + round(nrow(df) * rate)), ]
    })
}

onerow <- subsample(dataset, 0)
```

We have several different timing scenarios:

* `convert` is converting object to an R `raw()` and back.
* `connection` is also an in-memory conversion but using R
      textConnection or rawConnection objects.
* `file` writes to a temp file and then reads it back.
* `fifo` forks the R process and writes from one while reading
      from the other.
* `tcp` forks the R process and writes from one to the other using a
      TCP socket.

Each returns the OS-reported load for the process, elapsed time for
reading, writing and total. For `fifo` and `tcp` tests, the reading
and writing may happen concurrently, so that `total.elapsed <
read.elapsed + write.elapsed`.

## R serialization

`serialize` and `unserialize` produce faithful replications of R
objects including R-specific structures like closures, environments,
and attributes. But generally only R code can read it. It may be a
good candidate for communicating between R processes.

For instance:

```{R cache=TRUE}
unlist(timeConvert(dataset, unserialize, function(data) serialize(data, NULL)))
```

This is reasonably speedy, too.

However, I run into a problem if I try to transfer too large an object over
a fifo or socket and read it with `unserialize().`

```{R}
unserialize.bad <- timeFifoTransfer(dataset, unserialize, serialize)
dput(unserialize.bad$extra, control=c())
```

This appears to be because `unserialize` doesn't operate concurrently,
in the sense that it doesn't recover from finding the end of the line
having only read part of a message. Meanwhile, on my machine `fifo()`
and `socketConnection()` do not seem to block even if `blocking =
TRUE` is set. They always return some number of bytes, but it may be
fewer than requested. So `unserialize` does not work with blocking
socket connections.

One workaround is to exhaustively read the connection before handing
off the data to `unserialize`.

```{R}
readAll <- function(con, chunk = 8192L) {
  buf <- rawConnection(raw(0), open="w")
  repeat {
    result <- readBin(con = con, what = "raw", n = chunk)
    if (length(result) == 0) break
    writeBin(result, buf)
  }
  rawConnectionValue(buf)
}

timeSocketTransfer(
  dataset,
  . %>% readAll %>% unserialize,
  serialize
) %>% unlist
```

But this strategy only works for transmitting one object per
connection, and for one connection at a time.

Another way you can do it is to wrap R serialization into into `msgpack`
connection. This only adds a few bytes to the connection, and msgpack
will handle assembling complete messages.

```{R}
timeRawConnection(
  dataset,
  reader = function(con) unserialize(readMsg(con)),
  writer = function(x, con) writeMsg(serialize(x, NULL), con),
  wrap = msgConnection
) %>% unlist
```

## Dput/source

The `dput` and `deparse` render R data objects to an ASCII
connection in R syntax. The mechanism for reading objects back in is
to `eval` or `source` the text.

    dput(list(1, "2", verb=quote(buckle), my=c("s", "h", "o", "e")))

Performance-wise, `dput` and `source` should not be used for large
datasets, because they display an O(n^2) characteristic in terms of
the data size:

```{R}
library(ggplot2)
## Annoyingly, the behavior of "dump" and "dput" depend on this global option.
options(deparse.max.lines = NULL)

deparse.curve <- timeCurve(dataset, timeConvert, deparse, function(t) eval(parse(text=t)))
```

```{R}
dataset <- rbind.fill(dataset, deparse.curve)
ggplot(deparse.curve) + aes(x=size, y=total.elapsed) + geom_line()
```

## jsonlite

```{R, cache=TRUE}
library(jsonlite)

json <- timeCurve(dataset, timeConvert, fromJSON, toJSON)
```

jsonlite performs reasonably well, but is about N times slower than
serializing.

```{R}
dataset <- rbind.fill(dataset, json)
ggplot(json) + aes(x=size, y=total.elapsed) + geom_line()

```
`jsonlite` supports reading to or writing from connections, but only
to transmit one data frame at a time.

```{R}
jsonlite_reader <- function(conn) {
  append <- msgpack:::catenator()
  stream_in(conn, handler = function(x) append(list(x)))
  append(action="read")
}

jsonlite_writer <- function(data, conn) {
  lapply(data, stream_out, conn)
}

json <- timeCurve(dataset, timeConvert, fromJSON, toJSON)
timeSocketTransfer(dataset, reader=jsonlite_reader, writer=jsonlite_writer)
```

```{R}

ggplot(json) + aes(x=size, y=total.elapsed) + geom_line()
```


# Msgpack

Now `msgpack` appears to have some streaming issues with especially large messages.

```{R cache = TRUE}
library(msgpack)

timeConvert(dataset, unpackMsg, packMsg)

## note that msgpack requires a wrapper over the raw connection in order
## to pull off partial reads.
msgpack.raw <- timeCurve(dataset,
                         start=0.001,
                         method = timeConvert,
                         timeout = 5,
                         reader = unpackMsg,
                         writer = packMsg,
                         wrap = msgConnection)

ggplot(msgpack.raw) + aes(x=size, y=total.elapsed) + geom_line()

msgpack.rawconn <- timeCurve(dataset,
                             start=0.01,
                             method = timeRawConnection,
                             reader = readMsg,
                             writer = writeMsg,
                             wrap = msgConnection,
                             timeout = 10)

ggplot(msgpack.rawconn) + aes(x=size, y=total.elapsed) + geom_line()
```

Implementing callbacks has helped a lot, but there is still an O(N^2)
characteristic going on here. Need to profile where it is allocating
its memory.

```{R cache = TRUE}

msgpack.socket <- timeCurve(dataset,
                            start=0.001,
                            method = timeFifoTransfer,
                            reader = readMsg,
                            writer = writeMsg,
                            wrap = msgConnection, timeout=10)

timeRawConnection(dataset, writer = writeMsg, reader = readMsg, wrap = msgConnection)
timeFileIO(dataset, writer = writeMsg, reader = readMsg, wrap = msgConnection)
timeFifoTransfer(dataset, writer = writeMsg, reader = readMsg, wrap = msgConnection)
timeSocketTransfer(dataset, writer = writeMsg, reader = readMsg, wrap = msgConnection)
```

## msgpackR

There is an older pure-R implementation of msgpack on CRAN. One quirk is that
it doesn't handle `NA` in R vectors.

```{R cache=TRUE}
msgpackR::pack(c(1, 2, 3))
msgpackR::pack(c(1, 2, 3, NA))
```

As a workaround I'll substitute out all the NA values in the dataset.

```{R}
library(purr)
dataset_mungenull <- map(dataset, map_dfc,
                         function(col) ifelse(is.na(col), 9999, col))
```

Performance-wise, it is mostly linear but quite slow.


```{R cache=TRUE}
old_msgpack <- timeCurve(dataset_mungenull,
                         timeConvert, msgpackR::unpack, msgpackR::pack,
                         start = 0.001)
```

```{R cache=TRUE}
ggplot(old_msgpack) + aes(x=size, y=total.elapsed) + geom_line()
```

## rjson

rjson is rather zippy -- only ___ times slower than msgpack :)

```{R cache=TRUE}
rjson <- timeCurve(dataset,
                   timeConvert, rjson::fromJSON, rjson::toJSON,
                   start = 0.001)

timeConvert(dataset, rjson::fromJSON, rjson::toJSON)
timeConvert(dataset, unpackMsg, packMsg)
```

## RJSONIO
```{R cache=TRUE}
rjson <- timeCurve(dataset, unpack) 
```

## RJSONIO

```{R cache=TRUE}
timeCurve(dataset, timeConvert, RJSONIO::fromJSON, RJSONIO::toJSON) 
timeConvert(dataset, RJSONIO::fromJSON, RJSONIO::toJSON) 
```

RJSONIO also includes some support for streaming:

```{R cache=FALSE}
x <- RJSONIO::toJSON(dataset)

timeTextConnection(dataset,
                   RJSONIO::readJSONStream,
                   function(x, con)write(con, RJSONIO::toJSON(x)))
```

Be careful with giant strings of JSON in interactive mode...

## ndjson

ndjson is a "wicked-fast" streaming JSON reader, but not a
writer.

```{R cache = TRUE}
library(ndjson)

```

```{R cache = TRUE}

```

## write.csv

CSV is ubiquitous if not all that efficient or safe.

## write.csv

CSV is ubiquitous if not all that efficient or safe.

## And collecting all the data
