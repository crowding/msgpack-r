#---
#title: The Fastest Way to get data in to or out of R?
#output: rmarkdown::html_vignette
#vignette: >
#  %\VignetteEngine{knitr::rmarkdown}
#  %\VignetteIndexEntry{Speed of R Import and Export data in Various Packages}
#  \usepackage[utf8]{inputenc}
#---

It's good to get data into and out of R! More compatibility between
different formats is better. But... which formats or packages are fastest?

First, we will need a reference dataset to work with. I'll pick the
NYC flights dataset since it's large enough to take some time to
process.

Next, we'll pick some metrics. Here are a few I am interested in.

* wall clock time from beginning to end of task;
* CPU process time used as recorded by the OS
* total time spent to encode (to memory)
* total time spent to decode (from memory)
* total time spent to encode, transmit to another process on the same
  computer, and decode
* total number of bytes used
* all the above but with a layer of gzip used on top of the encoding

Here are all the libraries we will deal with. But since some of the
names clobber others I will have to load and install them in segments.

NOTE: also investigate YAML read/write, and HTML read/write?

Note that this is not a completely apples-to-apples comparison, since
the different formats have different capabilities and
purposes:

* `serialize()` is your best best for getting R objects out of
  and back into R the way you had them.
* `dump()` is your next best bet, while being a somewhat
  human-readable text format.
* JSON is widely used on the web, but because it is based on
  Javascript data structures, it doesn't easily represent all of R
  losslessly; attributes like dims and classes don't have equivalents
  in Javascript.
* CSV is ubiquitous and can be read by most anything but only represents
  tabular data. Its data types are ambiguous.
* Msgpack is similar in capabilities to JSON but is a binary format
  with more compact and faster encoding.
* Some of these packages encode data frames by column (more compact)
  and others encode data frames by row (more common with JSON APIs.)

Here's my strategy for benchmarking writing over connections. I will
write the dataset, from one forked R process to another on the same
machine. The dataset will be loaded into memory before the child
process is forked, then the child process encodes and writes to the
master process, which decodes. Timings are taken on both sides of the
connection, and we can examine the roundtrip-formatted data in the
master process afterwards.

The implementation of the benchmarking code is in file `benchmarks.R`.

```{R cache=FALSE}
read_chunk('benchmarking.R', labels = "setup")
opts_chunk$set(cache=TRUE, autodep=TRUE, message = FALSE, warning = FALSE)
```
As a test dataset we'll use `nycflights13`. Since the packages vary so
much in performance, we'll have to dynamically vary the input size. So
the function `subsample` produces an artificially shrunk or enlarged
version of the dataset according to the argument `rate`.

```{R cache=FALSE}
read_chunk('benchmarking.R', labels = "dataset")
```

We have several different timing scenarios:

* `convert` is converting object to an R `raw()` and back.
* `con` is also an in-memory conversion but using R
      `textConnection` or `rawConnection` objects.
* `file` writes to a temp file and then reads it back.
* `fifo` forks the R process and writes from one while reading
      from the other.
* `tcp` forks the R process and writes from one to the other using a
      TCP socket (on the same machine)
* `net` (TODO) sends the encoded text to another R process to decode.

```{R cache = FALSE}
read_chunk('benchmarking.R', labels = "definitions")
```

Each returns the OS-reported load for the process, elapsed time for
reading, writing and total. For `fifo` and `tcp` tests, the reading
and writing may happen concurrently, so that `total.elapsed <
read.elapsed + write.elapsed`.

## R serialization

`serialize` and `unserialize` produce faithful replications of R
objects including R-specific structures like closures, environments,
and attributes. But generally only R code can read it. It may be a
good candidate for communicating between R processes.

For instance:

```{R}
unlist(timeConvert(dataset, unserialize, function(data) serialize(data, NULL)))
```

This is reasonably speedy, too.

However, I run into a problem if I try to transfer too large an object over
a fifo or socket and read it with `unserialize().`

```{R}
unserialize.bad <- timeFifoTransfer(dataset, unserialize, serialize)
dput(unserialize.bad$extra, control=c())
```

This appears to be because `unserialize` doesn't operate concurrently,
in the sense that it doesn't recover from finding the end of the line
having only read part of a message. Meanwhile, on my machine `fifo()`
and `socketConnection()` do not seem to block even if `blocking =
TRUE` is set. They always return some number of bytes, but it may be
fewer than requested. So `unserialize` does not work with blocking
socket connections.

One workaround is to exhaustively read the connection before handing
off the data to `unserialize`.

```{R}
readAll <- function(con, chunk = 8192L) {
  buf <- rawConnection(raw(0), open="w")
  repeat {
    result <- readBin(con = con, what = "raw", n = chunk)
    if (length(result) == 0) break
    writeBin(result, buf)
  }
  rawConnectionValue(buf)
}

timeSocketTransfer(
  dataset,
  . %>% readAll %>% unserialize,
  serialize
) %>% unlist
```

But this strategy only works for transmitting one object per
connection, and for one connection at a time.

Another way you can do it is to wrap R serialization into into `msgpack`
connection. This only adds a few bytes to the connection, and msgpack
will handle assembling complete messages.

```{R}
timeRawConnection(
  dataset,
  reader = function(con) unserialize(readMsg(con)),
  writer = function(x, con) writeMsg(serialize(x, NULL), con),
  wrap = msgConnection
) %>% unlist
```

(TODO: This ought to be faster.)

Now let's start collecting benchmarks systematically.

```{R}
serialize.results <- run_tests(arg_df(c(
  all.common.options,
  list(
    note = list("using readAll on connections" = list()),
    encoder = list(
      serialize = list(
        reader = function(c) unserialize(readAll(c)),
        writer = serialize,
        from = unserialize,
        to = function(data) serialize(data, NULL)))))))
```
```{R}
store(serialize.results)
```

## Dput/source

The `dput` and `deparse` render R data objects to an ASCII connection
in R-like syntax.  THe idea is that the dext output "looks like" the
code it takes to construct the object, to the extent that the
mechanism for reading objects back in is to `eval` or `source` the
text. (Hopefully one does not do this with untrusted data. A better
technique may be to evaluate the data in a limited environment that
just contains the needed constructors like `structure`, `list` and `c`
etc.)

```{R}
## Annoyingly, the behavior of "dump" and "dput" depend on this global option.
options(deparse.max.lines = NULL)

deparse(control=c(),
        list(1, "2", verb=quote(buckle), my=c("s", "h", "o", "e")) )
```

Performance-wise, `dput` and `source` should not be used for large
datasets, because they display an O(n^2) characteristic in terms of
the data size.

```{R}
library(ggplot2)

deparse.curve <- timeCurve(dataset, timeConvert, deparse, function(t) eval(parse(text=t)))
```

```{R}
ggplot(deparse.curve) + aes(x=size, y=total.elapsed) + geom_line()
```

```{R}
options(deparse.max.lines = NULL)
dput.timings <- run_tests(arg_df(c(
  all.common.options,
  list(
    encoder = list(
      dput = list(
        from = function(t) eval(parse(text=t)),
        to = deparse,
        reader = function(c) eval(parse(c)),
        writer = function(x, c) dput(x, file=c)))))))
```

```{R}
store(dput.timings)
```

## jsonlite

```{R}
# `jsonlite` supports streaming reads but only of one data frame at a time, so...
jsonlite_reader <- function(conn) {
  append <- msgpack:::catenator()
  stream_in(conn, handler = function(x) append(list(x)))
  append(action="read")
}

jsonlite_writer <- function(l, conn) {
  lapply(l, stream_out, conn)
}

jsonlite.timings <- run_tests(arg_df(c(
  convert.common.options,
  list(
    encoder = list(
      jsonlite = list(
        to = jsonlite::toJSON,
        from = jsonlite::fromJSON,
        reader = jsonlite_reader,
        writer = jsonlite_writer))))))
```

jsonlite performs reasonably well, but is `{R} "several"` times slower than
serializing.

```{R}
store(jsonlite.timings)
```

# Msgpack

Msgpack is quite zippy but has some troubling quadratic behavior...

```{R cache = TRUE}
msgpack.timings <- run_tests(arg_df(c(
  all.common.options,
  list(
    encoder = list(
      msgpack = list(
        wrap = msgConnection,
        reader = msgpack::readMsg,
        writer = msgpack::writeMsg,
        to = msgpack::packMsg,
        from = msgpack::unpackMsg))))))
```

```{R}
store(msgpack.timings)
```

Implementing callbacks has helped a lot, but there is still an O(N^2)
characteristic going on here. Need to profile where it is allocating
its memory.

## msgpackR

There is an older pure-R implementation of msgpack on CRAN. One quirk is that
it doesn't accept `NA` in R vectors.

```{R}
msgpackR::pack(c(1, 2, 3))
msgpackR::pack(c(1, 2, 3, NA))
```

As a workaround I'll substitute out all the NA values in the dataset.

```{R}
dataset_mungenull <- map(dataset, map_dfc,
                         function(col) ifelse(is.na(col), 9999, col))
```

Performance-wise, it is mostly linear but quite slow.

```{R cache=TRUE}
msgpackR.timings <- run_tests(arg_df(c(
  all.common.options %but% list( 
    note = list("no NAs, readAll" = list()),
    dataset = list(nycflights13 = list(data = dataset_mungenull))),
  list(
    encoder = list(
      msgpackR = list(
        from = msgpackR::unpack,
        to = msgpackR::pack,
        reader = function(conn) msgpackR::unpack(readAll(conn)),
        writer = function(data, conn) writeBin(msgpackR::pack(data), conn)))))))
```

```{R}
store(msgpackR.timings)
```

```{R cache=TRUE}
ggplot(old_msgpack) + aes(x=size, y=total.elapsed) + geom_line()
```

## rjson

```{R cache=TRUE}
rjson.timings <- run_tests(arg_df(c(
  convert.common.options,
  list(
    encoder = list(
      rjson = list(
        from = rjson::fromJSON,
        to = rjson::toJSON))))))
```

rjson is rather zippy -- only `{R "several"}` times slower than msgpack :)

```{R}
store(rjson.timings)
```

## RJSONIO

```{R}
RJSONIO.timings <- run_tests(arg_df(c(
  all.common.options,
  list(
    encoder = list(
      RJSONIO = list(
        from = RJSONIO::fromJSON,
        to = RJSONIO::toJSON,
        reader = RJSONIO::readJSONStream,
        writer = function(x, con)writeBin(RJSONIO::toJSON(x), con),
        raw = TRUE))))))
```

```{R}
store(RJSONIO.timings)
```

```{R cache=TRUE}
timeCurve(dataset, timeConvert, RJSONIO::fromJSON, RJSONIO::toJSON)
timeConvert(dataset, RJSONIO::fromJSON, RJSONIO::toJSON)
```

RJSONIO also includes some support for streaming:

```{R cache=FALSE}
x <- RJSONIO::toJSON(dataset)

y <- RJSONIO::readJSONStream(textConnection(x, open="r"))

# The "read stream" function does not work with textConnections. Odd.
timeRawConnection(dataset,
           RJSONIO::readJSONStream,
           function(x, con)writeBin(RJSONIO::toJSON(x), con))
```

Be careful with giant strings of JSON in interactive mode...

## ndjson

ndjson is a "wicked-fast" streaming JSON reader, but not a
writer. So I will pair it with the fastest other one...

## write.csv

Do we really have to go here? I suppose...

## XML

